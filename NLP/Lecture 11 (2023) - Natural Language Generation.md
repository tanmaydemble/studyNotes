###### NLU vs NLG
Under NLU (understanding), the input is natural language and in NLG, the output is natural language. 
###### Categorization of NLG tasks by entropy
Tasks like machine translation and summarization have low entropy whereas story generation is high entropy as the output space is diverse, there is no one right answer.
###### Teacher forcing vs student forcing
In student forcing, the words generated by the model up till time t are given to the model to predict the word at time t + 1 whereas with teacher forcing, the correct text in the training data up till time t is given to predict the text at time t + 1.
###### Problem with open ended generation
The most likely string is repetitive for open ended generation. Hence, we can't use the most likely word that is predicted by the decoder.  Looking at decoder models, the negative log likelihood or the loss keeps on decreasing if we repeat the same phrase. This means that the model is more confident when repeatedly predicting the same phrase. There is a self-amplification effect.
###### How to reduce repetition
- Simple - don't repeat n-grams
- Complex - Use a different training objective
	- Unlikelihood objective: penalize generation of already-seen tokens - this decreases the probability of repeated generation at training time
	- Coverage loss: prevents attention mechanism from attending to the same words 
- Use a different decoding objective
	- contrastive decoding searches for strings x that maximize the log probability of a large model minus the log probability of a small model. so if both the models start repeating the same word then the value of this objective gets closer to 0 forcing the models to predict different words preventing repetition.  

Finding the most likely word after each step is not the best approach for open ended generation. 

#### Beam search
Beam search is a heuristic search algorithm used to find the most promising sequences in problems like natural language generation and speech recognition by expanding only a limited set of top candidates at every step, rather than exhaustively exploring all possibilities. This approach balances computational efficiency and solution quality, though it may sacrifice optimality and completeness depending on the chosen beam width.[](https://en.wikipedia.org/wiki/Beam_search)
###### How Beam Search Works
Beam search begins from an initial state and generates all possible successors, evaluating them via a heuristic (often a probability or score function). At every level, it keeps only a predetermined number (“beam width,” symbolically WW) of top candidates while discarding the rest. It repeats this expansion-selection cycle until a termination condition (like generating a complete sequence) is met or no further expansion is possible.[](https://www.geeksforgeeks.org/machine-learning/introduction-to-beam-search-algorithm/)
###### Key Characteristics
- Beam width (WW) controls memory use and accuracy: wider beams consider more candidate sequences and are less likely to prune optimal paths, but require more resources.[](https://en.wikipedia.org/wiki/Beam_search)
- The algorithm is greedy, meaning it may not always yield the globally best solution—just plausible candidates in large search spaces.[](https://www.geeksforgeeks.org/machine-learning/introduction-to-beam-search-algorithm/)
- Used in many AI systems, such as machine translation, speech recognition, and sequence decoding for NLP tasks, to approximate results that would be costly to obtain exhaustively.
#### Sampling
###### Vanilla sampling
Makes every token in the vocabulary a viable option. Even is most of the probability mass in the distribution is over a limited set of options, the tail of the distribution could be very long and in aggregate have a considerable mass. 
###### Top k sampling
Only sample from the top k tokens in the probability distribution - essentially cut the tail of the probability distribution - k is a hyperparameter which should be small for close ended tasks but bigger for open ended tasks. 
The problem with top k sampling is that it can cut off too quickly or too slowly. If it cuts off too quickly then there might be options we want to consider but they have been eliminated. This leads to poor recall as there are higher false negatives. The problem with cutting off too slowly is that there might be options that don't make sense but they are still included in the sample. This leads to poor precision as the false positives are higher. 
###### Top p sampling
Sample from all tokens in the top p cumulative probability mass. Allows for a more flexible k depending on the distribution. If we have a flatter distribution then we use a higher k and if we have a light tailed distribution then we select smaller k. This method sacrifices compute for higher performance as we have to calculate probability over the entire vocabulary.
###### Typical sampling
reweights the score based on the entropy of the distribution. Provides smaller entropy for close ended tasks and higher entropy for open ended tasks. 
###### Epsilon sampling
If a word has less probability than a particular threshold then it will never appear as the next word. 
#### Scaling randomness: Temperature
![[Pasted image 20250920210611.png]]
#### Re-ranking
![[Pasted image 20250920211805.png]]
#### Exposure bias
Training with teacher forcing leads to exposure bias at generation time. During training, our model's inputs are gold context tokens from real, human-generated texts. At generation time, our model's inputs are previously-decoded tokens. Some common solutions to exposure bias are:
###### Scheduled sampling
![[Pasted image 20250920213202.png]]
###### Dataset aggregation
![[Pasted image 20250920213232.png]]
###### Retrieval Augmented Generation
![[Pasted image 20250920213742.png]]
###### Reinforcement Learning
Treat the text generation model as a Markov decision process. 
State s is the model's representation of the preceding context
Actions a are the words that can be generated
Policy pi is the decoder 
Rewards r are provided by an external source

To define a reward function, just use the evaluation metric:
BLEU score for machine translation, ROUGE for summarization, CIDEr for image captioning.
![[Pasted image 20250920214314.png]]
Use human preference as a reward function. Just ask humans to rank a piece of generated text. Learn a reward function of the human preference. 
#### Evaluation methods for text generation
##### Content overlap metrics
Compute a score based on the lexical similarity between generated and gold standard text. Think BLEU and ROUGE score.
##### Word overlap based metrics
Think n gram overlap metrics. The only problem is that this method might miss sentences that have the same meaning because the words used are different or might actually show that sentences are similar because the words are the same even though the meaning is different. 
This metric is worse for open ended tasks. 
N grams don't have any concept of semantic relatedness. 
![[Pasted image 20250920220541.png]]
##### Model based metrics
Use learned representations of words and sentences to compute semantic similarity between generated and reference texts. The embeddings are pretrained and distance metrics can be used to measure the similarity.
###### Vector similarity
Embedding based similarity for semantic distance between text
###### Word Mover's distance
Measures the distance between two sequences using word embedding similarity matching. 
###### BERTSCORE
Use pretrained contextual embeddings from BERT and match words in candidate and reference sentences by cosine similarity. 




